---
date: 2024-02-22 02:01:35
AutoNoteMover: disable
---

## Logs
- [[옵시디언 RAG 시스템 구축하기 1]]

## Notes
- [[옵시디언 워크플로우에 도움이 되는 플러그인]]
- [[맥에서 폴더 간 동기화 방법]]
- 올라마에서 Hugging Face 언어 모델 실행 방법
	- GGUF 형식이라면 올라마에서 바로 실행할 수 있습니다.
	- 또는 [convert](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py)를 사용해 Hugging Face에서 한국어 모델을 다운로드한 후 도커 볼륨을 마운트하고 `python convert-hf-to-gguf.py` 명령어로 실행하여 GGUF 형식으로 변환할 수 있습니다.
		- 참고: https://github.com/ggerganov/llama.cpp/discussions/2948
- #뇌과학 [하버드대 뇌과학자의 깨달음 - YouTube](https://youtu.be/mDPZwjqlsmk) → [요약노트](https://lilys.ai/digest/302235?videoId=mDPZwjqlsmk&result=blogPost&source=video)
	- 하버드대 뇌과학자 테일러는 뇌졸중으로 인해 좌뇌 기능이 손상되며 자신의 정체성과 신체 경계를 잃었다.
	- 그러나 뇌의 가소성 덕분에 다른 부분이 기능을 대신하며 8년간 회복되었다. 
	- 테일러는 그 경험을 통해 우리가 우주의 일부이며, 고통에서 벗어나 평온을 찾는 첫 걸음은 부정적 사고 회로에서 벗어나는 것이라는 것을 깨달았다.
- #뇌과학 #뇌가소성 [세계 석학들 “매일 이 5가지만 지켜도 1.5배 똑똑해집니다” - YouTube](https://www.youtube.com/watch?v=cAaMzYMtFEw)
	- 유산소 운동, 읽기, 중얼거리기 등의 활동을 통해 뇌를 활성화시켜 능력을 향상시킬 수 있다.
	- 뇌 훈련을 통해 나이와 상관없이 뇌를 젊게 유지하고, 노화를 늦출 수 있다. 
	- 의지와 노력으로 뇌 손상 회복이 가능하며, 고령자의 경우에도 뇌 훈련으로 발전할 수 있다.

## Resources
- #개발  [shadcn/ui](https://ui.shadcn.com): 재사용 가능한 컴포넌트 라이브러리 ([[2024-02-06#^4v4p|HeadlessCSS]])
	- [shadcn/ui 를 이용한 공유 UI 컴포넌트 사용 경험](https://velog.io/@ckstn0777/shadcnui-%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EA%B3%B5%EC%9C%A0-UI-%EC%BB%B4%ED%8F%AC%EB%84%8C%ED%8A%B8-%EC%82%AC%EC%9A%A9-%EA%B2%BD%ED%97%98)
- #개발 [🦋 Changeset을 활용한 모노레포 자동 배포 구축하기](https://jinyisland.kr/post/changeset)
	- Changeset는 다중 패키지 저장소에 중점을 두고 버전 관리 및 변경 로그를 관리하는 도구
	- [pnpm에서 changesets 사용하기](https://pnpm.io/ko/using-changesets)
- #개발 [TSUP: TypeScript 를 위한 번들러 | 커리어리](https://careerly.co.kr/comments/94982)
	- **TypeScript 라이브러리의 번들링**을 위한 도구
	- **복잡한 설정 없이도 간편하게 사용**할 수 있는 번들러.
	- Go 언어로 개발되어 굉장히 빠른 속도를 자랑하는 **esbuild**에 의해 구동.
- [Reddit - Dive into anything](https://www.reddit.com/r/nextjs/comments/13owssn/supabase_or_planetscale)/
- #AI 위키피디아 임베딩 모델: https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3
- #AI/LLM 구글에서 새로운 언어 모델을 공개 Gemma7B: [Gemma model available in Vertex AI and via GKE | Google Cloud Blog](https://cloud.google.com/blog/products/ai-machine-learning/gemma-model-available-in-vertex-ai-and-via-gke)
	- HuggingFace: https://huggingface.co/google/gemma-7b
- #AI/LLM/한국어 #LLaVA https://github.com/tabtoyou/KoLLaVA
- #AI MS AutoGen: https://microsoft.github.io/autogen/
- #AI [LLMLingua](https://github.com/microsoft/LLMLingua): LLM의 추론 속도를 높이고 주요 정보에 대한 LLM의 인식을 개선. 프롬프트와 KV 캐시를 압축하여 성능 손실을 최소화. 최대 20배의 압축을 달성.
- #AI #perplexity https://labs.perplexity.ai/
- #AI/RAG #Mistral 쿡북: https://github.com/mistralai/cookbook/ #북마크
- #AI/LLM 버클리 대학에서 라마2 기반으로 Gemini 1.5와 동일하게 1M 토큰 처리가 가능한 모델을 공개
	- 논문: https://arxiv.org/html/2402.08268v1
	- HuggingFace: https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M
- #AI/LLM/한국어 [한국어 잘하는 오픈소스 모델 Orion 14B](https://www.linkedin.com/posts/seungyun-baek-aa40a4211_%EB%B0%B1%EC%8A%B9%EC%9C%A4-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%98%ED%95%98%EB%8A%94-%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-%EB%AA%A8%EB%8D%B8-orion-14b-%EB%93%B1%EC%9E%A5-%EC%BB%A4%EB%A6%AC%EC%96%B4%EB%A6%AC-activity-7155892775060705280-vqhD/) #북마크
	- 논문:  https://arxiv.org/abs/2401.12246
	- HuggingFace:
		- Base: https://huggingface.co/OrionStarAI/Orion-14B-Base
		- RAG: https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG
- #AI [AI, 또 혁명이 일어났습니다... 챗GPT 20배 속도 ㄷㄷㄷ  1초에 400단어 쏟아지는 LLM 등장! 지금 바로 쓸 수 있습니다 - YouTube](https://www.youtube.com/watch?v=HHlDuZW5zTg)
	- Google의 TPU 설계자 Jonathan Ross가 설립한 AI 반도체 스타트업 Groq.
	- Meta의 LLaMa2와 Mistral의 Mixtral8x7B 모델 사용.
	- 1초에 400~500단어 출력 가능한 서비스 공개. 기존 챗GPT와 Gemini에 비해 현저히 빠른 속도.
	- 특정 LLM을 위한 하드웨어 가속과 작업 스케줄링으로 최적화된 Language Processing Unit 결과물.
	- 속도의 혁신으로 다양한 응용처 발굴 가능. AI 분야에 큰 변화 예상.

